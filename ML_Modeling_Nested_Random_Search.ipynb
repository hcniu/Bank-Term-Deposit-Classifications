{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,KFold,RandomizedSearchCV,GridSearchCV\n",
    "from sklearn import metrics,svm\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import lightgbm as lgb\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ML Modeling Overview ###\n",
    "In this section, I am going to test our various ML models, such as LightGBM, XGBoost, and Random Forest. To find out the best performing ML model, I will furst perform **nested random search** to find the overall performance model. Next, I will use **random search** to tune the hyperparameters of the best model. Lastly, I will evaluate the final model's performance on test data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Load the train and test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/haochunniu/Desktop/Kaggle Compatition/Bank Term Deposit Predictions NN ML Nested Random Search/data/train_preprocessed.csv\")\n",
    "test = pd.read_csv(\"/Users/haochunniu/Desktop/Kaggle Compatition/Bank Term Deposit Predictions NN ML Nested Random Search/data/test_preprocessed.csv\")\n",
    "\n",
    "y_train, y_test = train['y'], test['y']\n",
    "x_train = train.drop(columns=['y'])\n",
    "x_test = test.drop(columns=['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Compute class weight ###\n",
    "Given that the data is extremely imbalanced ( y = 1 : 11.7%, y = 0 : 88.3% ), we need to modify the class weight to emphasize more on the classification ability on the minority ( y = 1 )."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.566241671258955, 1: 4.274059368500661}\n"
     ]
    }
   ],
   "source": [
    "class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "print(class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.56624167 0.56624167 0.56624167 ... 4.27405937 0.56624167 0.56624167]\n"
     ]
    }
   ],
   "source": [
    "# Unlike other models, XGBoost model use sample weights not class weights. So, this section is used to calculate sample weights\n",
    "xgb_class_weights = class_weight.compute_sample_weight(class_weight='balanced',y=y_train)\n",
    "print(xgb_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Nested random search to find the overall best model ###\n",
    "To find the overall best model, I use the nested random search method to find out which model with hyperparameter can be different on each cross validation split would be the best. Given that our data is inbalanced, I used **F1 score** as evalustion metrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "XGBoost start\n",
      "XGboost ends and take 63 sec.\n",
      "Random Forest start\n",
      "Random Forest ends and take 293 sec.\n",
      "Logistic Regression start\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:1113: UserWarning: Setting penalty='none' will ignore the C and l1_ratio parameters\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression ends and take 6 sec.\n",
      "LightGBM start\n",
      "LightGBM ends and take 14 sec.\n",
      "XGBoost Nested CV F1 weighted: 86.81 %\n",
      "Random Forest Nested CV F1 weighted: 85.16 %\n",
      "Logistic Regression Nested CV F1 weighted: 85.27 %\n",
      "LightGBM Nested CV F1 weighted: 85.96 %\n"
     ]
    }
   ],
   "source": [
    "# 1. Create the Classifier\n",
    "xgb=XGBClassifier(objective=\"binary:logistic\",seed=9,use_label_encoder =False,\n",
    "                  scale_pos_weight=(88.3/11.7))\n",
    "\n",
    "rf=RandomForestClassifier(random_state=9,class_weight=class_weights)\n",
    "\n",
    "log = LogisticRegression(class_weight=class_weights,max_iter=500,random_state=9)\n",
    "\n",
    "lgbm=lgb.LGBMClassifier(objective='binary',random_state=9,class_weight=class_weights)\n",
    "\n",
    "##############################################################\n",
    "# 2. Create the parameter grid\n",
    "xgb_grid={'eta':np.arange(0.1,3,0.1),\n",
    "          'max_depth':list(range(3,10)),\n",
    "          'n_estimators':list(range(10,100,10)),\n",
    "          'gamma':list(range(1,6)) }\n",
    "\n",
    "rf_grid={'n_estimators':list(range(100,500,100)),\n",
    "         'max_depth':list(range(3,10))}\n",
    "\n",
    "log_grid={'penalty':['l2','none'],\n",
    "          'C':[0.1,0.5,1.0,3.0,5.0]}\n",
    "\n",
    "lgbm_grid={'learning_rate':np.arange(0.1,3,0.1),\n",
    "           'max_depth':list(range(3,10)),\n",
    "           'n_estimators':list(range(10,100,10))}\n",
    "\n",
    "##############################################################\n",
    "# 3. Create the CV\n",
    "inner_cv = KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "outer_cv = KFold(n_splits=3, shuffle=True, random_state=9)\n",
    "\n",
    "##############################################################\n",
    "#In this case, because we are dealing with multi-class classification problem, we need to select a method to average our scoring metrics for the CVs\n",
    "#So, in this case, we use both weighted f1 for both outer and inner CV\n",
    "# 4-1-1. Random-search CV for XGBoost\n",
    "t1 = time.time()\n",
    "print(\"XGBoost start\")\n",
    "clf = RandomizedSearchCV(xgb,xgb_grid,cv=inner_cv,scoring='f1_weighted',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-1-2. Nested CV for XGBoost\n",
    "nested_score = cross_val_score(clf,X=x_train, y=y_train, cv=outer_cv,scoring='f1_weighted') \n",
    "\n",
    "# 4-1-3. Result for Nested CV\n",
    "xgb_result=nested_score.mean()\n",
    "t2 = time.time()\n",
    "print(\"XGboost ends and take {} sec.\".format(round(t2-t1)))\n",
    "\n",
    "##############################################################\n",
    "# 4-2-1. Random-search CV for Random Forest\n",
    "t1 = time.time()\n",
    "print(\"Random Forest start\")\n",
    "clf = RandomizedSearchCV(rf,rf_grid,cv=inner_cv,scoring='f1_weighted',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-2-2. Nested CV for Random Forest\n",
    "nested_score = cross_val_score(clf,X=x_train, y=y_train, cv=outer_cv,scoring='f1_weighted')\n",
    "\n",
    "# 4-2-3. Result for Nested CV\n",
    "rf_result=nested_score.mean()\n",
    "t2 = time.time()\n",
    "print(\"Random Forest ends and take {} sec.\".format(round(t2-t1)))\n",
    "\n",
    "##############################################################\n",
    "# 4-3-1. Random-search CV for Logistic Classifier\n",
    "t1 = time.time()\n",
    "print(\"Logistic Regression start\")\n",
    "clf = RandomizedSearchCV(log,log_grid,cv=inner_cv,scoring='f1_weighted',n_iter=4,random_state=9)\n",
    "\n",
    "# 4-3-2. Nested CV for LightGBM Classifier\n",
    "nested_score = cross_val_score(clf,X=x_train, y=y_train, cv=outer_cv,scoring='f1_weighted')\n",
    "\n",
    "# 4-3-3. Result for Nested CV\n",
    "log_result=nested_score.mean()\n",
    "t2 = time.time()\n",
    "print(\"Logistic Regression ends and take {} sec.\".format(round(t2-t1)))\n",
    "\n",
    "##############################################################\n",
    "# 4-4-1. Random-search CV for LightGBM Classifier\n",
    "t1 = time.time()\n",
    "print(\"LightGBM start\")\n",
    "clf = RandomizedSearchCV(lgbm,lgbm_grid,cv=inner_cv,scoring='f1_weighted',n_iter=15,random_state=9)\n",
    "\n",
    "# 4-4-2. Nested CV for LightGBM Classifier\n",
    "nested_score = cross_val_score(clf,X=x_train, y=y_train, cv=outer_cv,scoring='f1_weighted')\n",
    "\n",
    "# 4-4-3. Result for Nested CV\n",
    "lgbm_result=nested_score.mean()\n",
    "t2 = time.time()\n",
    "print(\"LightGBM ends and take {} sec.\".format(round(t2-t1)))\n",
    "\n",
    "##############################################################\n",
    "print(\"XGBoost Nested CV F1 weighted:\",round(xgb_result*100,2),\"%\")\n",
    "print(\"Random Forest Nested CV F1 weighted:\",round(rf_result*100,2),\"%\")\n",
    "print(\"Logistic Regression Nested CV F1 weighted:\",round(log_result*100,2),\"%\")\n",
    "print(\"LightGBM Nested CV F1 weighted:\",round(lgbm_result*100,2),\"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Random search to find the best XGBoost model ###\n",
    "To find the best XGboost model, I use the random search method to find out which hyperparameter combination is the bast for XGBoost model. Given that our data is inbalanced, I used **F1 score** as evalustion metrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Create the Classifier\n",
    "xgb=XGBClassifier(objective=\"binary:logistic\",seed=9,use_label_encoder =False,\n",
    "                  scale_pos_weight=(88.3/11.7))\n",
    "\n",
    "##############################################################\n",
    "# 2. Create the parameter grid\n",
    "xgb_grid={'eta':np.arange(0.1,3,0.1),\n",
    "          'max_depth':list(range(3,10)),\n",
    "          'n_estimators':list(range(10,100,10)),\n",
    "          'gamma':list(range(1,6)) }\n",
    "\n",
    "##############################################################\n",
    "# 3. Start the random search\n",
    "xgb_model = RandomizedSearchCV(xgb,xgb_grid,cv=3,scoring='f1_weighted',n_iter=15,random_state=9)\n",
    "\n",
    "# 4. Fit the model\n",
    "xgb_model.fit(x_train,y_train,sample_weight=xgb_class_weights)\n",
    "\n",
    "# 5. Predict\n",
    "y_pred=xgb_model.predict(x_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With CV random search, I found the best hyperparameter is eta=1.5000000000000002, max_depth=9, n_estimators=60, and gamma=2.\n",
      "----------------------------------------------------------------------------------------------------------------\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.97      0.98      4000\n",
      "           1       0.80      1.00      0.89       521\n",
      "\n",
      "    accuracy                           0.97      4521\n",
      "   macro avg       0.90      0.98      0.94      4521\n",
      "weighted avg       0.98      0.97      0.97      4521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 6. Result\n",
    "print (\"With CV random search, I found the best hyperparameter is eta={}, max_depth={}, n_estimators={}, and gamma={}.\".format(xgb_model.best_params_['eta'],\n",
    "                                                                                                                               xgb_model.best_params_['max_depth'],\n",
    "                                                                                                                               xgb_model.best_params_['n_estimators'],\n",
    "                                                                                                                               xgb_model.best_params_['gamma']))\n",
    "print('----------------------------------------------------------------------------------------------------------------')\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Final_XGBoost.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the final XGboost model\n",
    "import joblib\n",
    "joblib.dump(xgb_model.best_estimator_, 'Final_XGBoost.pkl')\n",
    "\n",
    "#Load the model\n",
    "#xgb_model = joblib.load(\"Final_XGBoost.pkl\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
