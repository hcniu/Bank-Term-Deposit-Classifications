{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_addons/utils/tfa_eol_msg.py:23: UserWarning: \n",
      "\n",
      "TensorFlow Addons (TFA) has ended development and introduction of new features.\n",
      "TFA has entered a minimal maintenance and release mode until a planned end of life in May 2024.\n",
      "Please modify downstream libraries to take dependencies from other repositories in our TensorFlow community (e.g. Keras, Keras-CV, and Keras-NLP). \n",
      "\n",
      "For more information see: https://github.com/tensorflow/addons/issues/2807 \n",
      "\n",
      "  warnings.warn(\n",
      "/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/tensorflow_addons/utils/ensure_tf_install.py:53: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.10.0 and strictly below 2.13.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.9.1 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import cross_val_score, train_test_split,KFold\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import recall_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "from sklearn.utils import class_weight\n",
    "from sklearn.preprocessing import LabelEncoder, MinMaxScaler\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "from tensorflow import keras\n",
    "from keras.utils import np_utils\n",
    "from keras import layers\n",
    "import keras_tuner\n",
    "from keras_tuner.tuners import RandomSearch\n",
    "from keras import backend as K\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Activation, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Network Modeling Overview ###\n",
    "In this section, I will try 3, 4, and 5 layers feed forward NN models. For each model, I will use random search to find the best hyperparameters for each model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step1: Load the train and test data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"/Users/haochunniu/Desktop/Kaggle Compatition/Bank Term Deposit Classifications NN ML Nested Random Search/data/train_preprocessed.csv\")\n",
    "test = pd.read_csv(\"/Users/haochunniu/Desktop/Kaggle Compatition/Bank Term Deposit Classifications NN ML Nested Random Search/data/test_preprocessed.csv\")\n",
    "\n",
    "y_train, y_test = train['y'], test['y']\n",
    "x_train = train.drop(columns=['y'])\n",
    "x_test = test.drop(columns=['y'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step2: Calculate class weight for NN ###\n",
    "Given that the data is extremely imbalabced, we also need to set class weight to force more focus on the minorities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 0.566241671258955, 1: 4.274059368500661}\n"
     ]
    }
   ],
   "source": [
    "nn_class_weights = compute_class_weight(class_weight = 'balanced',classes = np.unique(y_train),y = y_train)\n",
    "nn_class_weights = dict(zip([0,1], nn_class_weights))\n",
    "print(nn_class_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step3: Random search on 3 layers NN model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 02m 29s]\n",
      "val_loss: 0.15603642165660858\n",
      "\n",
      "Best val_loss So Far: 0.048848044127225876\n",
      "Total elapsed time: 00h 15m 30s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Random Search on 3 Layers Neural Network\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu']),\n",
    "                           input_dim=x_train.shape[1]))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [0.01, 0.005,0.001])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner=RandomSearch(build_model,\n",
    "                   objective=\"val_loss\",\n",
    "                   max_trials=5,\n",
    "                   overwrite=True,\n",
    "                   seed=9,\n",
    "                   executions_per_trial=1)\n",
    "\n",
    "#Keras cannot input object data type, so no matter the column is boolean or numeric we need to transform them to float32\n",
    "x_train_float = np.asarray(x_train).astype(np.float32)\n",
    "x_test_float = np.asarray(x_test).astype(np.float32)\n",
    "\n",
    "\n",
    "tuner.search(x=x_train_float,\n",
    "             y=y_train,\n",
    "             epochs=500,\n",
    "             batch_size=512,\n",
    "             validation_data=(x_test_float,y_test),\n",
    "             class_weight = nn_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best 3 layers DNN parameters would be 300 neurons, relu as activation function, and 0.001 learning rate.\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result=tuner.get_best_hyperparameters()[0].values\n",
    "print('The best 3 layers DNN parameters would be {} neurons, {} as activation function, and {} learning rate.'.format(result['units'],result['activation'],result['learning_rate']))\n",
    "#print('------------------------------------------')\n",
    "#print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 901us/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      4000\n",
      "           1       0.85      0.98      0.91       521\n",
      "\n",
      "    accuracy                           0.98      4521\n",
      "   macro avg       0.92      0.98      0.95      4521\n",
      "weighted avg       0.98      0.98      0.98      4521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the final model\n",
    "Three_layers_NN=tuner.get_best_models()[0]\n",
    "\n",
    "#Get the predicton on validation data\n",
    "y_pred=Three_layers_NN.predict(x_test_float)\n",
    "y_pred=np.round(y_pred)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://02ffb0fd-026a-4301-80b6-4073e2dae4e6/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Final_3LayersNN.pkl']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the final 3 layers NN model\n",
    "import joblib\n",
    "joblib.dump(Three_layers_NN, 'Final_3LayersNN.pkl')\n",
    "\n",
    "#Load the model\n",
    "#Three_layers_NN = joblib.load(\"Final_3LayersNN.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step4: Random search on 4 layers NN model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 04m 03s]\n",
      "val_loss: 0.15244600176811218\n",
      "\n",
      "Best val_loss So Far: 0.03336123377084732\n",
      "Total elapsed time: 00h 23m 13s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Random Search on 4 Layers Neural Network\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu']),\n",
    "                           input_dim=x_train.shape[1]))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [0.01, 0.005,0.001])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner=RandomSearch(build_model,\n",
    "                   objective=\"val_loss\",\n",
    "                   max_trials=5,\n",
    "                   overwrite=True,\n",
    "                   seed=9,\n",
    "                   executions_per_trial=1)\n",
    "\n",
    "#Keras cannot input object data type, so no matter the column is boolean or numeric we need to transform them to float32\n",
    "x_train_float = np.asarray(x_train).astype(np.float32)\n",
    "x_test_float = np.asarray(x_test).astype(np.float32)\n",
    "\n",
    "\n",
    "tuner.search(x=x_train_float,\n",
    "             y=y_train,\n",
    "             epochs=500,\n",
    "             batch_size=512,\n",
    "             validation_data=(x_test_float,y_test),\n",
    "             class_weight = nn_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best 4 layers DNN parameters would be 300 neurons, relu as activation function, and 0.001 learning rate.\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result=tuner.get_best_hyperparameters()[0].values\n",
    "print('The best 4 layers DNN parameters would be {} neurons, {} as activation function, and {} learning rate.'.format(result['units'],result['activation'],result['learning_rate']))\n",
    "#print('------------------------------------------')\n",
    "#print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 2ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.98      0.99      4000\n",
      "           1       0.88      1.00      0.93       521\n",
      "\n",
      "    accuracy                           0.98      4521\n",
      "   macro avg       0.94      0.99      0.96      4521\n",
      "weighted avg       0.99      0.98      0.98      4521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the final model\n",
    "Four_layers_NN=tuner.get_best_models()[0]\n",
    "\n",
    "#Get the predicton on validation data\n",
    "y_pred=Four_layers_NN.predict(x_test_float)\n",
    "y_pred=np.round(y_pred)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://2adca448-7549-4dbc-bf9a-2240fefef010/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Final_4LayersNN.pkl']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the final 4 layers NN model\n",
    "import joblib\n",
    "joblib.dump(Four_layers_NN, 'Final_4LayersNN.pkl')\n",
    "\n",
    "#Load the model\n",
    "#Fourlayers_NN = joblib.load(\"Final_4LayersNN.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step5: Random search on 5 layers NN model ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 05m 58s]\n",
      "val_loss: 0.1476597934961319\n",
      "\n",
      "Best val_loss So Far: 0.02121235430240631\n",
      "Total elapsed time: 00h 36m 47s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Random Search on 5 Layers Neural Network\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu']),\n",
    "                           input_dim=x_train.shape[1]))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(units=hp.Int('units',\n",
    "                                        min_value=100,\n",
    "                                        max_value=300,\n",
    "                                        step=50),\n",
    "                           activation=hp.Choice('activation',values = ['sigmoid','relu'])))\n",
    "    model.add(layers.Dense(1,activation='sigmoid'))\n",
    "\n",
    "    hp_learning_rate = hp.Choice('learning_rate', values = [0.01, 0.005,0.001])\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                  loss=keras.losses.BinaryCrossentropy(),\n",
    "                  metrics=['accuracy'])\n",
    "    return model\n",
    "\n",
    "tuner=RandomSearch(build_model,\n",
    "                   objective=\"val_loss\",\n",
    "                   max_trials=5,\n",
    "                   overwrite=True,\n",
    "                   seed=9,\n",
    "                   executions_per_trial=1)\n",
    "\n",
    "#Keras cannot input object data type, so no matter the column is boolean or numeric we need to transform them to float32\n",
    "x_train_float = np.asarray(x_train).astype(np.float32)\n",
    "x_test_float = np.asarray(x_test).astype(np.float32)\n",
    "\n",
    "\n",
    "tuner.search(x=x_train_float,\n",
    "             y=y_train,\n",
    "             epochs=600,\n",
    "             batch_size=512,\n",
    "             validation_data=(x_test_float,y_test),\n",
    "             class_weight = nn_class_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The best 5 layers DNN parameters would be 300 neurons, relu as activation function, and 0.001 learning rate.\n"
     ]
    }
   ],
   "source": [
    "# Result\n",
    "result=tuner.get_best_hyperparameters()[0].values\n",
    "print('The best 5 layers DNN parameters would be {} neurons, {} as activation function, and {} learning rate.'.format(result['units'],result['activation'],result['learning_rate']))\n",
    "#print('------------------------------------------')\n",
    "#print(tuner.results_summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "142/142 [==============================] - 0s 1ms/step\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      1.00      4000\n",
      "           1       0.93      1.00      0.96       521\n",
      "\n",
      "    accuracy                           0.99      4521\n",
      "   macro avg       0.97      1.00      0.98      4521\n",
      "weighted avg       0.99      0.99      0.99      4521\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Get the final model\n",
    "Five_layers_NN=tuner.get_best_models()[0]\n",
    "\n",
    "#Get the predicton on validation data\n",
    "y_pred=Five_layers_NN.predict(x_test_float)\n",
    "y_pred=np.round(y_pred)\n",
    "\n",
    "print(classification_report(y_test,y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: ram://47d80936-55fc-4975-ae3e-2ef1c7ba9072/assets\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['Final_5LayersNN.pkl']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Save the final 5 layers NN model\n",
    "import joblib\n",
    "joblib.dump(Five_layers_NN, 'Final_5LayersNN.pkl')\n",
    "\n",
    "#Load the model\n",
    "#Fivelayers_NN = joblib.load(\"Final_5LayersNN.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion: ###\n",
    "Among all machine learning models, the XGBoost model with ETA = 1.5, max_depth = 9, n_estimators = 60, and gamma = 2 is the best model. In the test data, the model could capture all postivie cases but ***the precision is quite bad with only 80%***. On the other hand, the ***5 layers NN model with 300 neurons, relu as activation function, and 0.001 learning rate*** is the best NN model. The model does a fantastic job that it not only could ***capture all postivie cases*** but also could ***boost the precision up to 93%*** in test data. Thus, the ***final 5 layers NN model*** is the best classification model."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
